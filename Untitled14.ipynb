{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVSzVjVigrIQojJ9CaAN+u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scandy-dxd/scandy-dxd/blob/main/Untitled14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKvD8H0nCX_w",
        "outputId": "2e684c8b-316a-45cf-b9ac-f9e52f9d1d02"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.29.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (42.0.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pdfplumber\n",
        "import spacy  # For NLP tasks\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "cULITVlQ7y22"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_nlp_model():\n",
        "    \"\"\"Loads the spaCy NLP model for text processing.\"\"\"\n",
        "    nlp = spacy.load(\"en_core_web_sm\")  # Consider a larger model for better accuracy if needed\n",
        "    return nlp\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Preprocesses text for NLP tasks: lowercase conversion and tokenization.\"\"\"\n",
        "    nlp = load_nlp_model()\n",
        "    doc = nlp(text)\n",
        "    return [token.text.lower() for token in doc]\n"
      ],
      "metadata": {
        "id": "Sodbv0KF73yq"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extracts text from a PDF document using pdfplumber.\"\"\"\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            policy_text = \"/content/sample_data/Company-Policy-and-Procedure-June-1.18-V6.0.pdf\"\n",
        "            for page in pdf.pages:\n",
        "                policy_text += page.extract_text()\n",
        "        return policy_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from PDF: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "4hjn-MSp84Pe"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_candidate_rules(policy_text):\n",
        "    \"\"\"Extracts candidate rules (conditions and actions) from policy text using NLP.\"\"\"\n",
        "    nlp = load_nlp_model()  # Load the NLP model here\n",
        "    nlp_doc = nlp(policy_text)  # Preprocess and tokenize the policy text\n",
        "    candidate_rules = []\n",
        "\n",
        "    for token in nlp_doc:\n",
        "        # Example rule condition: \"harassment\" and \"report\" but not \"supervisor\" or \"HR\"\n",
        "        if token.text == \"harassment\" and \"report\" in [t.text for t in nlp_doc]:\n",
        "            if \"supervisor\" not in [t.text for t in nlp_doc] and \"HR\" not in [t.text for t in nlp_doc]:\n",
        "                condition = \"If a sentence mentions 'harassment' and 'report' but not 'supervisor' or 'HR'\"\n",
        "                action = \"it suggests a potential policy violation.\"\n",
        "                candidate_rules.append({\"condition\": condition, \"action\": action})\n",
        "\n",
        "        return candidate_rules\n",
        "\n"
      ],
      "metadata": {
        "id": "2WxtnBUL85-s"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_rules_with_model(model, tokenizer, rules, user_prompt, policy_sections):\n",
        "    \"\"\"Evaluates rules using the sentence transformer model and similarity scores.\"\"\"\n",
        "    encoded_user_prompt = tokenizer(user_prompt, return_tensors=\"pt\")\n",
        "    user_prompt_embedding = model(**encoded_user_prompt).pooler_output\n",
        "\n",
        "    rule_evaluations = []\n",
        "    for rule in rules:\n",
        "        condition_snippets = [f\"I was {rule['condition']}.\"]  # Generate snippets for each condition\n",
        "        rule_similarities = []\n",
        "        for snippet in condition_snippets:\n",
        "            encoded_snippet = tokenizer(snippet, return_tensors=\"pt\")\n",
        "            snippet_embedding = model(**encoded_snippet).pooler_output\n",
        "            similarity = torch.nn.functional.cosine_similarity(user_prompt_embedding, snippet_embedding).item()\n",
        "            rule_similarities.append(similarity)\n",
        "\n",
        "                  # Consider averaging similarities or using a more robust aggregation method\n",
        "        average_similarity = sum(rule_similarities) / len(rule_similarities)\n",
        "        rule_evaluations.append({\n",
        "            \"rule\": rule,\n",
        "            \"average_similarity\": average_similarity,\n",
        "            \"policy_sections\": evaluate_rule_in_policy(policy_sections, rule[\"condition\"])  # Check in policy sections\n",
        "        })\n",
        "\n",
        "    return rule_evaluations\n",
        "\n"
      ],
      "metadata": {
        "id": "pkUZlFHh_fhh"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_rule_in_policy(policy_sections, rule_condition):\n",
        "    \"\"\"Checks if the rule condition is mentioned in relevant policy sections (optional, can be customized).\"\"\"\n",
        "    # This is a simplified example. You might need more sophisticated comparison based on NLP tasks.\n",
        "    matches = []\n",
        "    for section in policy_sections:\n",
        "        if rule_condition in section:\n",
        "            matches.append(section)\n",
        "    return matches\n",
        "\n"
      ],
      "metadata": {
        "id": "gPP8Yhtv_0UO"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def refine_and_integrate_rules(rule_evaluations, threshold=0.7):\n",
        "    \"\"\"Refines and integrates rules based on similarity scores and manual review.\"\"\"\n",
        "    refined_rules = []\n",
        "    for evaluation in rule_evaluations:\n",
        "        if evaluation[\"average_similarity\"] > threshold and evaluation[\"policy_sections\"]:  # Consider both similarity and presence in policy\n",
        "            print(f\"Rule: {evaluation['rule']['condition']}\\nAction: {evaluation['rule']['action']}\")\n",
        "            print(f\"Similarity: {evaluation['average_similarity']}\\nPolicy Sections: {evaluation['policy_sections']}\")\n",
        "            user_confirmation = input(\"Is this rule valid (y/n)? \")\n",
        "            if user_confirmation.lower() == \"y\":\n",
        "                refined_rules.append(evaluation[\"rule\"])\n",
        "\n",
        "    return refined_rules"
      ],
      "metadata": {
        "id": "yEYxW4WD_5ZG"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage (replace with your actual model, tokenizer, and PDF path)\n",
        "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "pdf_path = \"/content/sample_data/Company-Policy-and-Procedure-June-1.18-V6.0.pdf\"\n",
        "\n",
        "# Extract text from the PDF\n",
        "policy_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "# Extract candidate rules from the policy text\n",
        "if policy_text:\n",
        "    rules = extract_candidate_rules(str(policy_text))\n",
        "\n",
        "    # Assuming you have a way to obtain relevant policy sections (manually or through parsing)\n",
        "    policy_sections = [\"HEALTH LITERACY \"]  # Replace with actual sections\n",
        "\n",
        "    # Evaluate rules with the model and refine them\n",
        "    if rules:\n",
        "        rule_evaluations = evaluate_rules_with_model(model, tokenizer, rules, user_prompt=\"What language should consumer materials be in ?.\",policy_sections=policy_sections)\n",
        "        refined_rules = refine_and_integrate_rules(rule_evaluations)\n",
        "\n",
        "        if refined_rules:\n",
        "            print(\"Refined Rules:\")\n",
        "            for rule in refined_rules:\n",
        "                print(f\"- {rule['condition']}: {rule['action']}\")\n",
        "        else:\n",
        "            print(\"No refined rules generated based on the current settings.\")\n",
        "    else:\n",
        "        print(\"No candidate rules extracted from the policy text.\")\n",
        "else:\n",
        "    print(\"Error extracting text from the PDF.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7VWepqWBkqQ",
        "outputId": "4d8cb18d-60d2-4acd-847f-80155d7468f9"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No candidate rules extracted from the policy text.\n"
          ]
        }
      ]
    }
  ]
}